{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python')\n",
    "from get_vs import *\n",
    "from misc import *\n",
    "from models import *\n",
    "from mcmc import *\n",
    "\n",
    "import random\n",
    "import subprocess\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pangeo_data = True # True if in Pangeo binder, False if in laptop\n",
    "if is_pangeo_data:\n",
    "    n_workers = 10\n",
    "    cluster = KubeCluster(n_workers=n_workers)\n",
    "    client = Client(cluster)\n",
    "else:\n",
    "    cluster = None\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for cluster to be up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_pangeo_data:\n",
    "    def set_worker_env():\n",
    "        global mcmc\n",
    "        import sys, os\n",
    "        cwd = os.getcwd()\n",
    "        sys.path.append(cwd + '/python')\n",
    "        import mcmc\n",
    "    client.run(set_worker_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of the virtual stations in [Hydroweb](http://hydroweb.theia-land.fr) don't match with the rivers in [HydroSHEDS](http://www.hydrosheds.org). In order to find the corresponding coordinates in HydroSHEDS, we look around the original position for the pixel with the biggest accumulated flow which is bigger than a minimum flow. If no such flow is found, we look further around, until we find one (but not too far away, in which case we just drop the virtual station). The new_lat/new_lon are the coordinates of this pixel, if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/amazonas/amazonas.pkl'):\n",
    "    df = locate_vs('../data/amazonas/amazonas.txt', pix_nb=20, acc_min=1_000_000)\n",
    "    df.to_pickle('../data/amazonas/amazonas.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle('../data/amazonas/amazonas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_latlon = df[['new_lat', 'new_lon']].dropna().values\n",
    "print(f'Out of {len(df)} virtual stations in Hydroweb, {len(sub_latlon)} could be found in HydroSHEDS.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following coordinates are duplicated because some virtual stations fall inside the same pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_latlon = [(-4.928333333333334, -62.733333333333334), (-3.8666666666666667, -61.6775)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df[['new_lat', 'new_lon']].dropna()\n",
    "duplicated = df_ll[df_ll.duplicated(keep=False)]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the subbasins in the hydrologic partition (including virtual stations)\n",
    "#gcs_get_dir('pangeo-data/gross/ws_mask/amazonas', 'ws_mask/amazonas', fs)\n",
    "#gcs_w_token = gcsfs.GCSFileSystem(project='pangeo-data', token='browser')\n",
    "if is_pangeo_data:\n",
    "    fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "    labels = [os.path.basename(path[:-1]) for path in fs.ls('pangeo-data/gross/ws_mask/amazonas')]\n",
    "else:\n",
    "    labels = os.listdir('ws_mask/amazonas')\n",
    "print('Total number of subbasins:', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pickle_path = '../data/amazonas/labels.pkl'\n",
    "if not os.path.exists(label_pickle_path):\n",
    "    labels_without_vs = list(labels)\n",
    "    labels_with_vs = {}\n",
    "    for label in tqdm(labels):\n",
    "        ds = xr.open_zarr(f'ws_mask/amazonas/{label}')\n",
    "        da = ds['mask']\n",
    "        olat, olon = da.attrs['outlet']\n",
    "        idx = df_ll[(olat-0.25/1200<df_ll.new_lat.values) & (df_ll.new_lat.values<olat+0.25/1200) & (olon-0.25/1200<df_ll.new_lon.values) & (df_ll.new_lon.values<olon+0.25/1200)].index.values\n",
    "        if len(idx) > 0:\n",
    "            labels_without_vs.remove(label)\n",
    "            labels_with_vs[label] = list(df.iloc[idx].station.values)\n",
    "    with open(label_pickle_path, 'wb') as f:\n",
    "        pickle.dump((labels_with_vs, labels_without_vs), f)\n",
    "else:\n",
    "    with open(label_pickle_path, 'rb') as f:\n",
    "        labels_with_vs, labels_without_vs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_with_vs_tree = get_label_tree(list(labels_with_vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d1 = '2000-03-01 12:00:00', '2018-12-31'\n",
    "x_range = ((0.1, 1e4), (-1, 1), (0.1, 1e3), (0.1, 1e2))\n",
    "x_start = [[random.uniform(*rng) for rng in x_range] for _ in range(n_workers)]\n",
    "sample_nb = 1_000 # number of samples generated by MCMC\n",
    "burnin = sample_nb // 10 # number of burnin samples\n",
    "warmup = 30 * 24 * 2 # one month in 30min steps\n",
    "\n",
    "q_ensemble = {}\n",
    "#for ws_i in range(ws_nb):\n",
    "for label in labels_with_vs_tree:\n",
    "    # get basin's labels\n",
    "    ws_labels = startswith_label(label, labels)\n",
    "    # get basin's mask\n",
    "    os.makedirs('ws_mask/amazonas', exist_ok=True)\n",
    "    for l in ws_labels:\n",
    "        if not os.path.exists(f'ws_mask/amazonas/{l}'):\n",
    "            fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "            gcs_get_dir(f'pangeo-data/gross/ws_mask/amazonas/{l}', f'ws_mask/amazonas/{l}', fs)\n",
    "    da_mask = get_mask('ws_mask/amazonas', ws_labels)\n",
    "    subprocess.check_call('rm -rf mask'.split())\n",
    "    da_mask.to_dataset(name='mask').to_zarr('mask')\n",
    "    # get basin's precipitation and PET, and water level at virtual station\n",
    "    if is_pangeo_data:\n",
    "        p = get_precipitation(d0, d1, 'mask')\n",
    "        e = get_pet(d0, d1, 'mask')\n",
    "    else:\n",
    "        peq = pd.read_pickle('peq.pkl')\n",
    "        p = peq.p\n",
    "        e = peq.e\n",
    "    he = get_waterlevel(d0, d1, labels_with_vs[label][0]) # there might be several stations\n",
    "    peq = DataFrame()\n",
    "    peq['p'] = p\n",
    "    peq['e'] = e\n",
    "    peq['h_obs'] = he.h\n",
    "    peq['h_err'] = he.e\n",
    "    is_source_basin = True\n",
    "    if is_source_basin:\n",
    "        area_head = 1\n",
    "        area_tail = 0\n",
    "        # prior probability distribution is uniform for head basin\n",
    "        x0 = x_start\n",
    "        x_prior = [uniform_density(*r) for r in x_range]\n",
    "        lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_prior, x_range)]\n",
    "    else:\n",
    "        area_head = ws_i\n",
    "        area_tail = 1\n",
    "        x0 = [xy[0][np.argmax(xy[1])] for xy in x_head]\n",
    "        x0 += [d_start] + x_start\n",
    "        # prior probability distribution is uniform for tail basin\n",
    "        lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_head, x_range)]\n",
    "        lnprob_prior += [lnprob_from_density(uniform_density(*d_range), *d_range)]\n",
    "        x_tail = [uniform_density(*r) for r in x_range]\n",
    "        lnprob_prior += [lnprob_from_density(p, *r) for p, r in zip(x_tail, x_range)]\n",
    "    lnprob = get_lnprob(gr4hh, warmup, peq, lnprob_prior, area_head, area_tail)\n",
    "    # run MCMC\n",
    "    futures = [client.submit(mcmc.Sampler, x0[i], lnprob, actor=True) for i in range(n_workers)]\n",
    "    samplers = [future.result() for future in futures]\n",
    "    futures = [sampler.run(sample_nb, burnin) for sampler in samplers]\n",
    "    results = [future.result() for future in futures]\n",
    "    for i, result in enumerate(results):\n",
    "        if i == 0:\n",
    "            samples, q_sim = result\n",
    "        else:\n",
    "            samples = np.vstack((samples, result[0]))\n",
    "            q_sim += result[1]\n",
    "    sys.exit()\n",
    "    #sampler = Sampler(x0, lnprob)\n",
    "    #samples, q_sim = sampler.run(sample_nb, burnin)\n",
    "    # get simulated streamflow and uncertainty\n",
    "    q_sim = np.array(q_sim)\n",
    "    q_ensemble[f'f{ws_i}'] = q_sim\n",
    "    # plot updated streamflow\n",
    "    #plot_series(ensemble=q_sim[:, -days:], true=df[f'q_true_{ws_i}'].values[-days:], title=f'Streamflow at the outlet of $B_{ws_i}$')\n",
    "    if False:#(ws_i > 0) and (ws_i < ws_nb - 1):\n",
    "        # reduce dual model to single model\n",
    "        peq = df[['p', 'e']]\n",
    "        x_prior = [uniform_density(*r) for r in x_range]\n",
    "        lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_prior, x_range)]\n",
    "        q_kde = np.empty((2, n_kde, q_sim.shape[1]))\n",
    "        for i in range(q_kde.shape[2]):\n",
    "            q_kde[:, :, i] = get_kde(q_sim[:, i], nb=n_kde)\n",
    "        lnprob = get_lnprob(peq, lnprob_prior, 1, 0, q_kde)\n",
    "        x0 = x_start\n",
    "        sampler = mcmc.Sampler(x0, lnprob)\n",
    "        samples, q_sim = sampler.run(sample_nb, burnin)\n",
    "    x_head = [get_kde(samples[:, i]) for i in range(4)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
