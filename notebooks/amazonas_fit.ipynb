{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python')\n",
    "from get_vs import *\n",
    "from misc import *\n",
    "from models import *\n",
    "\n",
    "from mcmc import smc, dist\n",
    "import random\n",
    "import subprocess\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "from dask.distributed import Client\n",
    "\n",
    "is_pangeo_data = True # True if in Pangeo binder, False if in laptop\n",
    "if is_pangeo_data:\n",
    "    from dask_kubernetes import KubeCluster as Cluster\n",
    "    n_workers = 10\n",
    "else:\n",
    "    from dask.distributed import LocalCluster as Cluster\n",
    "    n_workers = 4\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for cluster to be up and running, then make some modules available on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_worker_env():\n",
    "    global mcmc\n",
    "    import sys, os\n",
    "    cwd = os.getcwd()\n",
    "    sys.path.append(cwd + '/python')\n",
    "    import mcmc\n",
    "client.run(set_worker_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of the virtual stations in [Hydroweb](http://hydroweb.theia-land.fr) don't match with the rivers in [HydroSHEDS](http://www.hydrosheds.org). In order to find the corresponding coordinates in HydroSHEDS, we look around the original position for the pixel with the biggest accumulated flow which is bigger than a minimum flow. If no such flow is found, we look further around, until we find one (but not too far away, in which case we just drop the virtual station). The new_lat/new_lon are the coordinates of this pixel, if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/amazonas/amazonas.pkl'):\n",
    "    df = locate_vs('../data/amazonas/amazonas.txt', pix_nb=20, acc_min=1_000_000)\n",
    "    df.to_pickle('../data/amazonas/amazonas.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle('../data/amazonas/amazonas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_latlon = df[['new_lat', 'new_lon']].dropna().values\n",
    "print(f'Out of {len(df)} virtual stations in Hydroweb, {len(sub_latlon)} could be found in HydroSHEDS.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following coordinates are duplicated because some virtual stations fall inside the same pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_latlon = [(-4.928333333333334, -62.733333333333334), (-3.8666666666666667, -61.6775)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df[['new_lat', 'new_lon']].dropna()\n",
    "duplicated = df_ll[df_ll.duplicated(keep=False)]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the subbasins in the hydrologic partition (including virtual stations)\n",
    "#gcs_get_dir('pangeo-data/gross/ws_mask/amazonas', 'ws_mask/amazonas', fs)\n",
    "#gcs_w_token = gcsfs.GCSFileSystem(project='pangeo-data', token='browser')\n",
    "if is_pangeo_data:\n",
    "    fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "    all_labels = [os.path.basename(path[:-1]) for path in fs.ls('pangeo-data/gross/ws_mask/amazonas')]\n",
    "else:\n",
    "    all_labels = os.listdir('ws_mask/amazonas')\n",
    "print('Total number of subbasins:', len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pickle_path = '../data/amazonas/labels.pkl'\n",
    "if not os.path.exists(label_pickle_path):\n",
    "    labels_without_vs = list(labels)\n",
    "    labels_with_vs = {}\n",
    "    if is_pangeo_data:\n",
    "        gcs_get_dir('pangeo-data/gross/ws_mask', 'ws_mask', fs)\n",
    "    for label in tqdm(labels):\n",
    "        ds = xr.open_zarr(f'ws_mask/amazonas/{label}')\n",
    "        da = ds['mask']\n",
    "        olat, olon = da.attrs['outlet']\n",
    "        idx = df_ll[(olat-0.25/1200<df_ll.new_lat.values) & (df_ll.new_lat.values<olat+0.25/1200) & (olon-0.25/1200<df_ll.new_lon.values) & (df_ll.new_lon.values<olon+0.25/1200)].index.values\n",
    "        if len(idx) > 0:\n",
    "            labels_without_vs.remove(label)\n",
    "            labels_with_vs[label] = list(df.iloc[idx].station.values)\n",
    "    with open(label_pickle_path, 'wb') as f:\n",
    "        pickle.dump((labels_with_vs, labels_without_vs), f)\n",
    "else:\n",
    "    with open(label_pickle_path, 'rb') as f:\n",
    "        labels_with_vs, labels_without_vs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_with_vs_tree = get_label_tree(list(labels_with_vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_map(x, y):\n",
    "    df = DataFrame({'x': x, 'y': y}).dropna()\n",
    "    x_sorted = np.sort(df.x.values)\n",
    "    y_sorted = np.sort(df.y.values)\n",
    "    return np.interp(x, x_sorted, y_sorted)\n",
    "\n",
    "def get_logp(gr4, warmup, peq, area_up, area_down, down_label):\n",
    "    def logp(xs):\n",
    "        q_sim = 0\n",
    "        ## upstream basins\n",
    "        #xs_up = {k:v for k, v in xs.items() if k != down_label}\n",
    "        #for label, x in xs_up.items():\n",
    "        #    # x includes the delay in the GR model\n",
    "        #    q_sim += gr4(x).run([peq[f'p{label}'].values, peq[f'e{label}'].values]) * area_up[label]\n",
    "        # downstream basin\n",
    "        q_sim += gr4(xs).run([peq[f'p{down_label}'].values, peq[f'e{down_label}'].values]) * area_down\n",
    "        q_sim /= sum(area_up.values()) + area_down\n",
    "        # observation is measured water level\n",
    "        h_obs = peq.h_obs.values\n",
    "        h_err = peq.h_err.values\n",
    "        h_sim = np.hstack((np.full(warmup, np.nan), dist_map(q_sim[warmup:], h_obs[warmup:])))\n",
    "        df = DataFrame({'h_sim': h_sim, 'h_obs': h_obs, 'h_err': h_err})[warmup:].dropna()\n",
    "        std2 = df.h_err * df.h_err\n",
    "        # must not have zero error on observation\n",
    "        min_std2 = np.max(std2) / 100\n",
    "        std2 = np.clip(std2, min_std2, None)\n",
    "        lp = np.sum(-np.square(df.h_sim.values - df.h_obs.values) / (2 * std2) - np.log(np.sqrt(2 * np.pi * std2))), q_sim\n",
    "        return lp\n",
    "    return logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp/precipitation', exist_ok=True)\n",
    "os.makedirs('tmp/pet', exist_ok=True)\n",
    "os.makedirs('ws_mask/amazonas', exist_ok=True)\n",
    "\n",
    "d0, d1 = '2000-03-01 12:00:00', '2018-12-31'\n",
    "x_range = ((0.1, 1e4), (-1, 1), (0.1, 1e3), (0.1, 1e2))\n",
    "x_scale = [100, 0.1, 10, 1]\n",
    "#sample_nb = 1_000 # number of samples generated by MCMC\n",
    "#burnin = sample_nb // 10 # number of burnin samples\n",
    "draws = 100\n",
    "warmup = 12 * 30 * 24 * 2 # one year in 30min steps\n",
    "\n",
    "q_ensemble = {}\n",
    "for down_label in labels_with_vs_tree:\n",
    "    # get whole basin's labels\n",
    "    whole_labels = startswith_label(down_label, all_labels)\n",
    "    # copy basin's masks locally\n",
    "    for label in whole_labels:\n",
    "        if not os.path.exists(f'ws_mask/amazonas/{label}'):\n",
    "            fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "            gcs_get_dir(f'pangeo-data/gross/ws_mask/amazonas/{label}', f'ws_mask/amazonas/{label}', fs)\n",
    "    # get upstream basins' labels\n",
    "    # also compute upstream basins' areas\n",
    "    up_labels, area_up = {}, {}\n",
    "    for label in labels_with_vs_tree[down_label]['up']:\n",
    "        area_up[label] = 0\n",
    "        up_labels[label] = startswith_label(label, all_labels)\n",
    "        for label2 in up_labels[label]:\n",
    "            area_up[label] += xr.open_zarr(f'ws_mask/amazonas/{label2}', auto_chunk=False)['mask'].attrs['area']\n",
    "    # get downstream bassin's labels and compute its area\n",
    "    down_labels = whole_labels\n",
    "    for labels in up_labels.values():\n",
    "        down_labels = subtract_label(labels, down_labels)\n",
    "    area_down = 0\n",
    "    for label in down_labels:\n",
    "        area_down += xr.open_zarr(f'ws_mask/amazonas/{label}', auto_chunk=False)['mask'].attrs['area']\n",
    "    area_whole = sum(area_up.values()) + area_down\n",
    "\n",
    "    # get whole basin's mask\n",
    "    da = get_mask('ws_mask/amazonas', whole_labels)\n",
    "    subprocess.check_call('rm -rf tmp/mask'.split())\n",
    "    da.to_dataset(name='mask').to_zarr('tmp/mask')\n",
    "    # get basin's water level time series at virtual station (basin's outlet)\n",
    "    he = get_waterlevel(d0, d1, labels_with_vs[down_label][0]) # there might be several stations\n",
    "    dh0 = he.dropna().index[0] # first date of observation\n",
    "    dh1 = he.dropna().index[-1] # last date of observation\n",
    "    # start date which allows to warmup the model, but not more (warmup is in 30min)\n",
    "    dh0 = max(str2datetime(d0), dh0 - timedelta(minutes=warmup*30))\n",
    "    # get whole basin's precipitation and PET time series\n",
    "    if os.path.exists(f'tmp/precipitation/{down_label}.pkl'):\n",
    "        p = pd.read_pickle(f'tmp/precipitation/{down_label}.pkl')\n",
    "        e = pd.read_pickle(f'tmp/pet/{down_label}.pkl')\n",
    "    else:\n",
    "        print(f'Getting precipitation and PET for {down_label}')\n",
    "        p = get_precipitation(d0, d1, 'tmp/mask')\n",
    "        p.to_pickle(f'tmp/precipitation/{down_label}.pkl')\n",
    "        e = get_pet(d0, d1, 'tmp/mask')\n",
    "        e.to_pickle(f'tmp/pet/{down_label}.pkl')\n",
    "    p_up, e_up = {}, {}\n",
    "    # precipitation and PET of upstream bassins already exist from previous iteration\n",
    "    for label in labels_with_vs_tree[down_label]['up']:\n",
    "        p_up[label] = pd.read_pickle(f'tmp/precipitation/{label}.pkl')\n",
    "        e_up[label] = pd.read_pickle(f'tmp/pet/{label}.pkl')\n",
    "    # compute precipitation and PET of downstream bassin\n",
    "    p_down = p * area_whole\n",
    "    e_down = e * area_whole\n",
    "    for label in labels_with_vs_tree[label]['up']:\n",
    "        p_down -= p_up[label] * area_up[label]\n",
    "        e_down -= e_up[label] * area_up[label]\n",
    "    p_down /= area_whole\n",
    "    e_down /= area_whole\n",
    "\n",
    "    # create a DataFrame for whole basin's precipitation, PET and water level\n",
    "    peq = DataFrame()\n",
    "    peq[f'p{down_label}'] = p_down.loc[dh0:dh1]\n",
    "    peq[f'e{down_label}'] = e_down.loc[dh0:dh1]\n",
    "    peq[f'h_obs'] = he.h.loc[dh0:dh1]\n",
    "    peq[f'h_err'] = he.e.loc[dh0:dh1]\n",
    "\n",
    "    ## initial model parameter values with no prior information\n",
    "    #x_start = [[random.uniform(*rng) for rng in x_range] for _ in range(n_workers)]\n",
    "    #d_start = [random.uniform(0, 100) for _ in range(n_workers)]\n",
    "\n",
    "    if not up_labels:\n",
    "        # this is a source basin (no basin flowing into it)\n",
    "        area_up = {}\n",
    "        area_down = 1\n",
    "        # prior probability distribution is uniform for basin\n",
    "        x_pdf = [dist.uniform_pdf(*r) for r in x_range]\n",
    "        def prior_logp(values):\n",
    "            logp = sum([dist.logp_from_pdf(pdf, v) for pdf, v in zip(x_pdf, values)])\n",
    "            return logp\n",
    "        model_logp = get_logp(gr4hh, warmup, peq, area_up, area_down, down_label)\n",
    "    #else:\n",
    "    #    # there are basins flowing into this basin\n",
    "    #    x0 = []\n",
    "    #    for _ in range(n_workers):\n",
    "    #        # initial model parameter values are the most likely for head basins\n",
    "    #        _x0 = [sample(xyz[2]) for xyz in x_head for x_head in x_heads] # GR parameters\n",
    "    #        _x0 += [random.uniform(0, 100) for _ in x_heads] # delay (TODO: based on river length)\n",
    "    #        # initial model parameter values with no prior information for tail basin\n",
    "    #        _x0 += [random.uniform(*rng) for rng in x_range]\n",
    "    #        x0.append(_x0)\n",
    "    #    # prior probability distribution is uniform for tail basin\n",
    "    #    lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_head, x_range) for x_head in x_heads]\n",
    "    #    lnprob_prior += [lnprob_from_density(uniform_density(*d_range), *d_range) for _ in x_heads]\n",
    "    #    x_tail = [uniform_density(*r) for r in x_range]\n",
    "    #    lnprob_prior += [lnprob_from_density(p, *r) for p, r in zip(x_tail, x_range)]\n",
    "    # run SMC\n",
    "    posterior, q_sims = smc.smc(x_pdf, model_logp, prior_logp, draws=draws, dask_client=client)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for q_sim in q_sims:\n",
    "        plt.plot(dist_map(q_sim[warmup:], peq.h_obs.values[warmup:]), alpha=0.005, color='blue')\n",
    "    plt.scatter(range(len(peq)-warmup), peq.h_obs.values[warmup:])\n",
    "    plt.show()\n",
    "    #lnprob = mcmc.get_lnprob(gr4hh, warmup, peq, lnprob_prior, area_up, area_down)\n",
    "    ## run MCMC\n",
    "    #if True:#is_pangeo_data:\n",
    "    #    futures = [client.submit(mcmc.Sampler, x0[i], lnprob, scale=x_scale, actor=True) for i in range(n_workers)]\n",
    "    #    samplers = [future.result() for future in futures]\n",
    "    #    futures = [sampler.run(sample_nb, burnin) for sampler in samplers]\n",
    "    #    results = [future.result() for future in futures]\n",
    "    #    for i, result in enumerate(results):\n",
    "    #        if i == 0:\n",
    "    #            samples, q_sim = result\n",
    "    #        else:\n",
    "    #            samples = np.vstack((samples, result[0]))\n",
    "    #            q_sim += result[1]\n",
    "    #else:\n",
    "    #    sampler = mcmc.Sampler(x0, lnprob, scale=x_scale)\n",
    "    #    samples, q_sim = sampler.run(sample_nb, burnin)\n",
    "    #sys.exit()\n",
    "    # get simulated streamflow and uncertainty\n",
    "    #q_sim = np.array(q_sim)\n",
    "    #q_ensemble[f'f{label}'] = q_sim\n",
    "    # plot updated streamflow\n",
    "    #plot_series(ensemble=q_sim[:, -days:], true=df[f'q_true_{ws_i}'].values[-days:], title=f'Streamflow at the outlet of $B_{ws_i}$')\n",
    "    #if up_labels:\n",
    "    #    # reduce model\n",
    "    #    # get P and E over the whole basin\n",
    "    #    peq = df[['p', 'e']]\n",
    "    #    x_prior = [uniform_density(*r) for r in x_range]\n",
    "    #    lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_prior, x_range)]\n",
    "    #    q_kde = np.empty((2, n_kde, q_sim.shape[1]))\n",
    "    #    for i in range(q_kde.shape[2]):\n",
    "    #        q_kde[:, :, i] = get_kde(q_sim[:, i], nb=n_kde)\n",
    "    #    lnprob = get_lnprob(peq, lnprob_prior, 1, 0, q_kde)\n",
    "    #    x0 = x_start\n",
    "    #    sampler = mcmc.Sampler(x0, lnprob)\n",
    "    #    samples, q_sim = sampler.run(sample_nb, burnin)\n",
    "    #x_head = [get_kde(samples[:, i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
