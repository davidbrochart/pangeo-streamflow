{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python')\n",
    "from get_vs import *\n",
    "from misc import *\n",
    "from models import *\n",
    "from mcmc import *\n",
    "\n",
    "import subprocess\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "from dask_kubernetes import KubeCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pangeo_data = True # True if in Pangeo binder, False if in laptop\n",
    "if is_pangeo_data:\n",
    "    cluster = KubeCluster(n_workers=10)\n",
    "    client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of the virtual stations in [Hydroweb](http://hydroweb.theia-land.fr) don't match with the rivers in [HydroSHEDS](http://www.hydrosheds.org). In order to find the corresponding coordinates in HydroSHEDS, we look around the original position for the pixel with the biggest accumulated flow which is bigger than a minimum flow. If no such flow is found, we look further around, until we find one (but not too far away, in which case we just drop the virtual station). The new_lat/new_lon are the coordinates of this pixel, if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/amazonas/amazonas.pkl'):\n",
    "    df = locate_vs('../data/amazonas/amazonas.txt', pix_nb=20, acc_min=1_000_000)\n",
    "    df.to_pickle('../data/amazonas/amazonas.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle('../data/amazonas/amazonas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_latlon = df[['new_lat', 'new_lon']].dropna().values\n",
    "print(f'Out of {len(df)} virtual stations in Hydroweb, {len(sub_latlon)} could be found in HydroSHEDS.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following coordinates are duplicated because some virtual stations fall inside the same pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_latlon = [(-4.928333333333334, -62.733333333333334), (-3.8666666666666667, -61.6775)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df[['new_lat', 'new_lon']].dropna()\n",
    "duplicated = df_ll[df_ll.duplicated(keep=False)]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the subbasins in the hydrologic partition (including virtual stations)\n",
    "#gcs_get_dir('pangeo-data/gross/ws_mask/amazonas', 'ws_mask/amazonas', fs)\n",
    "#gcs_w_token = gcsfs.GCSFileSystem(project='pangeo-data', token='browser')\n",
    "if is_pangeo_data:\n",
    "    fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "    labels = [os.path.basename(path[:-1]) for path in fs.ls('pangeo-data/gross/ws_mask/amazonas')]\n",
    "else:\n",
    "    labels = os.listdir('ws_mask/amazonas')\n",
    "print('Total number of subbasins:', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pickle_path = '../data/amazonas/labels.pkl'\n",
    "if not os.path.exists(label_pickle_path):\n",
    "    labels_without_vs = list(labels)\n",
    "    labels_with_vs = {}\n",
    "    for label in tqdm(labels):\n",
    "        ds = xr.open_zarr(f'ws_mask/amazonas/{label}')\n",
    "        da = ds['mask']\n",
    "        olat, olon = da.attrs['outlet']\n",
    "        idx = df_ll[(olat-0.25/1200<df_ll.new_lat.values) & (df_ll.new_lat.values<olat+0.25/1200) & (olon-0.25/1200<df_ll.new_lon.values) & (df_ll.new_lon.values<olon+0.25/1200)].index.values\n",
    "        if len(idx) > 0:\n",
    "            labels_without_vs.remove(label)\n",
    "            labels_with_vs[label] = list(df.iloc[idx].station.values)\n",
    "    with open(label_pickle_path, 'wb') as f:\n",
    "        pickle.dump((labels_with_vs, labels_without_vs), f)\n",
    "else:\n",
    "    with open(label_pickle_path, 'rb') as f:\n",
    "        labels_with_vs, labels_without_vs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_with_vs_tree = get_label_tree(list(labels_with_vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mask = get_mask('ws_mask/amazonas', labels)\n",
    "da_mask.to_dataset(name='mask').to_zarr('mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d1 = '2000-03-01 12:00:00', '2018-12-31'\n",
    "precipitation = get_precipitation(d0, d1, 'mask', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pet = get_pet(d0, d1, 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame()\n",
    "df['p'] = precipitation\n",
    "df['e'] = pet\n",
    "df.plot(figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0, d1 = '2000-03-01 12:00:00', '2018-12-31'\n",
    "x_start = [100, 0, 10, 1]\n",
    "x_range = ((0.1, 1e4), (-1, 1), (0.1, 1e3), (0.1, 1e2))\n",
    "sample_nb = 2_000 # number of samples generated by MCMC\n",
    "burnin = sample_nb // 10 # number of burnin samples\n",
    "warmup = 30 * 24 * 2\n",
    "\n",
    "q_ensemble = {}\n",
    "#for ws_i in range(ws_nb):\n",
    "for label in labels_with_vs_tree:\n",
    "    # get basin's labels\n",
    "    ws_labels = startswith_label(label, labels)\n",
    "    # get basin's mask\n",
    "    os.makedirs('ws_mask/amazonas', exist_ok=True)\n",
    "    for l in ws_labels:\n",
    "        if not os.path.exists(f'ws_mask/amazonas/{l}'):\n",
    "            fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "            gcs_get_dir(f'pangeo-data/gross/ws_mask/amazonas/{l}', f'ws_mask/amazonas/{l}', fs)\n",
    "    da_mask = get_mask('ws_mask/amazonas', ws_labels)\n",
    "    subprocess.check_call('rm -rf mask'.split())\n",
    "    da_mask.to_dataset(name='mask').to_zarr('mask')\n",
    "    # get basin's precipitation and PET, and water level at virtual station\n",
    "    if is_pangeo_data:\n",
    "        p = get_precipitation(d0, d1, 'mask')\n",
    "        e = get_pet(d0, d1, 'mask')\n",
    "    else:\n",
    "        peq = pd.read_pickle('peq.pkl')\n",
    "        p = peq.p\n",
    "        e = peq.e\n",
    "    he = get_waterlevel(d0, d1, labels_with_vs[label][0]) # there might be several stations\n",
    "    peq = DataFrame()\n",
    "    peq['p'] = p\n",
    "    peq['e'] = e\n",
    "    peq['h_obs'] = he.h\n",
    "    peq['h_err'] = he.e\n",
    "    is_source_basin = True\n",
    "    if is_source_basin:\n",
    "        area_head = 1\n",
    "        area_tail = 0\n",
    "        # prior probability distribution is uniform for head basin\n",
    "        x0 = x_start\n",
    "        x_prior = [uniform_density(*r) for r in x_range]\n",
    "        lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_prior, x_range)]\n",
    "    else:\n",
    "        area_head = ws_i\n",
    "        area_tail = 1\n",
    "        x0 = [xy[0][np.argmax(xy[1])] for xy in x_head]\n",
    "        x0 += [d_start] + x_start\n",
    "        # prior probability distribution is uniform for tail basin\n",
    "        lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_head, x_range)]\n",
    "        lnprob_prior += [lnprob_from_density(uniform_density(*d_range), *d_range)]\n",
    "        x_tail = [uniform_density(*r) for r in x_range]\n",
    "        lnprob_prior += [lnprob_from_density(p, *r) for p, r in zip(x_tail, x_range)]\n",
    "    lnprob = get_lnprob(gr4hh, warmup, peq, lnprob_prior, area_head, area_tail)\n",
    "    # run MCMC\n",
    "    futures = [client.submit(mcmc.Sampler, x0, lnprob, actor=True) for _ in range(10)]\n",
    "    samplers = [future.result() for future in futures]\n",
    "    futures = [sampler.run(sample_nb, burnin) for sampler in samplers]\n",
    "    samples2, q_sim2 = [future.result() for future in futures]\n",
    "    sys.exit()\n",
    "    #sampler = Sampler(x0, lnprob)\n",
    "    #samples, q_sim = sampler.run(sample_nb, burnin)\n",
    "    # get simulated streamflow and uncertainty\n",
    "    q_sim = np.array(q_sim)\n",
    "    q_ensemble[f'f{ws_i}'] = q_sim\n",
    "    # plot updated streamflow\n",
    "    #plot_series(ensemble=q_sim[:, -days:], true=df[f'q_true_{ws_i}'].values[-days:], title=f'Streamflow at the outlet of $B_{ws_i}$')\n",
    "    if False:#(ws_i > 0) and (ws_i < ws_nb - 1):\n",
    "        # reduce dual model to single model\n",
    "        peq = df[['p', 'e']]\n",
    "        x_prior = [uniform_density(*r) for r in x_range]\n",
    "        lnprob_prior = [lnprob_from_density(p, *r) for p, r in zip(x_prior, x_range)]\n",
    "        q_kde = np.empty((2, n_kde, q_sim.shape[1]))\n",
    "        for i in range(q_kde.shape[2]):\n",
    "            q_kde[:, :, i] = get_kde(q_sim[:, i], nb=n_kde)\n",
    "        lnprob = get_lnprob(peq, lnprob_prior, 1, 0, q_kde)\n",
    "        x0 = x_start\n",
    "        sampler = mcmc.Sampler(x0, lnprob)\n",
    "        samples, q_sim = sampler.run(sample_nb, burnin)\n",
    "    x_head = [get_kde(samples[:, i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waterlevel(from_time, to_time, vs):\n",
    "    url = f'http://hydroweb.theia-land.fr/hydroweb/view/{vs}?lang=en&basin=AMAZONAS'\n",
    "    df, _, _ = get_vs(url)\n",
    "    # don't bother about 15min offset, nor about multiple values for a date\n",
    "    he = df.loc[from_time:to_time]\n",
    "    he.index = he.index.round('30min')\n",
    "    he = he.resample('30min').asfreq()\n",
    "    he.index = he.index + timedelta(minutes=15)\n",
    "    return he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lnprob(gr4, warmup, peq, lnprob_prior, area_head, area_tail, q_kde=None):\n",
    "    def lnprob(x):\n",
    "        lnp = 0\n",
    "        for i, v in enumerate(x):\n",
    "            lnp += lnprob_prior[i](v)\n",
    "        if not np.isfinite(lnp):\n",
    "            return -np.inf, np.ones_like(peq.p.values) * np.inf\n",
    "\n",
    "        x_head = x[:5] # this includes the delay in the gr4 model\n",
    "        g_head = gr4(x_head)\n",
    "        if area_tail > 0:\n",
    "            x_tail = x[5:]\n",
    "            g_tail = gr4(x_tail)\n",
    "            q_tail = g_tail.run([peq.p.values, peq.e.values])\n",
    "        else:\n",
    "            q_tail = 0\n",
    "        q_sim = (g_head.run([peq.p.values, peq.e.values]) * area_head + q_tail * area_tail) / (area_head + area_tail)\n",
    "        if q_kde is None:\n",
    "            # observation is measured water level\n",
    "            h_obs = peq.h_obs.values\n",
    "            h_err = peq.h_err.values\n",
    "            h_sim = np.hstack((np.full(warmup, np.nan), dist_map(q_sim[warmup:], h_obs[warmup:])))\n",
    "            df = DataFrame({'h_sim': h_sim, 'h_obs': h_obs, 'h_err': h_err})[warmup:].dropna()\n",
    "            std2 = df.h_err * df.h_err\n",
    "            return lnp + np.sum(-np.square(df.h_sim.values - df.h_obs.values) / (2 * std2) - np.log(np.sqrt(2 * np.pi * std2))), q_sim\n",
    "        else:\n",
    "            # observation is simulated streamflow\n",
    "            lnp_q = 0\n",
    "            for i in range(warmup, q_sim.size, sim_step):\n",
    "                lnp_q += lnprob_from_density(q_kde[:, :, i])(q_sim[i])\n",
    "            return lnp + lnp_q, q_sim\n",
    "    return lnprob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
