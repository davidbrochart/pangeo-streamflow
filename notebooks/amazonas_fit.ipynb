{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python')\n",
    "from virtual_stations import get_waterlevel\n",
    "from misc import get_precipitation, get_pet, get_label_tree, startswith_label, get_mask, get_masks, str2datetime, get_peq_from_df, gcs_get_dir\n",
    "from models import gr4hh\n",
    "from mcmc_utils import dist_map, get_likelihood_logp, get_prior_logp\n",
    "\n",
    "from mcmc import smc, dist\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import subprocess\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "from dask.distributed import Client\n",
    "\n",
    "is_pangeo_data = True # True if in Pangeo binder, False if in laptop\n",
    "if is_pangeo_data:\n",
    "    from dask_kubernetes import KubeCluster as Cluster\n",
    "    n_workers = 10\n",
    "else:\n",
    "    from dask.distributed import LocalCluster as Cluster\n",
    "    n_workers = 4\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../python/misc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    def set_worker_env():\n",
    "        global mcmc\n",
    "        import sys, os\n",
    "        cwd = os.getcwd()\n",
    "        sys.path.append(cwd + '/python')\n",
    "        import mcmc\n",
    "    client.run(set_worker_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates of the virtual stations in [Hydroweb](http://hydroweb.theia-land.fr) don't match with the rivers in [HydroSHEDS](http://www.hydrosheds.org). In order to find the corresponding coordinates in HydroSHEDS, we look around the original position for the pixel with the biggest accumulated flow which is bigger than a minimum flow. If no such flow is found, we look further around, until we find one (but not too far away, in which case we just drop the virtual station). The new_lat/new_lon are the coordinates of this pixel, if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/amazonas/amazonas.pkl'):\n",
    "    df_vs = locate_vs('../data/amazonas/amazonas.txt', pix_nb=20, acc_min=1_000_000)\n",
    "    df_vs.to_pickle('../data/amazonas/amazonas.pkl')\n",
    "else:\n",
    "    df_vs = pd.read_pickle('../data/amazonas/amazonas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_latlon = df_vs[['new_lat', 'new_lon']].dropna().values\n",
    "print(f'Out of {len(df_vs)} virtual stations in Hydroweb, {len(sub_latlon)} could be found in HydroSHEDS.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following coordinates are duplicated because some virtual stations fall inside the same pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_latlon = [(-4.928333333333334, -62.733333333333334), (-3.8666666666666667, -61.6775)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ll = df_vs[['new_lat', 'new_lon']].dropna()\n",
    "duplicated = df_ll[df_ll.duplicated(keep=False)]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the subbasins in the hydrologic partition (including virtual stations)\n",
    "#gcs_get_dir('pangeo-data/gross/ws_mask/amazonas', 'ws_mask/amazonas', fs)\n",
    "#gcs_w_token = gcsfs.GCSFileSystem(project='pangeo-data', token='browser')\n",
    "if is_pangeo_data:\n",
    "    fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "    all_labels = [os.path.basename(path[:-1]) for path in fs.ls('pangeo-data/gross/ws_mask/amazonas') if os.path.basename(path[:-1]).startswith('0')]\n",
    "else:\n",
    "    all_labels = [fname for fname in os.listdir('ws_mask/amazonas') if fname.startswith('0')]\n",
    "print('Total number of subbasins:', len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pickle_path = '../data/amazonas/labels.pkl'\n",
    "if not os.path.exists(label_pickle_path):\n",
    "    labels_without_vs = list(labels)\n",
    "    labels_with_vs = {}\n",
    "    if is_pangeo_data:\n",
    "        gcs_get_dir('pangeo-data/gross/ws_mask', 'ws_mask', fs)\n",
    "    for label in tqdm(labels):\n",
    "        ds = xr.open_zarr(f'ws_mask/amazonas/{label}')\n",
    "        da = ds['mask']\n",
    "        olat, olon = da.attrs['outlet']\n",
    "        idx = df_ll[(olat-0.25/1200<df_ll.new_lat.values) & (df_ll.new_lat.values<olat+0.25/1200) & (olon-0.25/1200<df_ll.new_lon.values) & (df_ll.new_lon.values<olon+0.25/1200)].index.values\n",
    "        if len(idx) > 0:\n",
    "            labels_without_vs.remove(label)\n",
    "            labels_with_vs[label] = list(df_vs.iloc[idx].station.values)\n",
    "    with open(label_pickle_path, 'wb') as f:\n",
    "        pickle.dump((labels_with_vs, labels_without_vs), f)\n",
    "else:\n",
    "    with open(label_pickle_path, 'rb') as f:\n",
    "        labels_with_vs, labels_without_vs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_with_vs_tree = get_label_tree(list(labels_with_vs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp/precipitation', exist_ok=True)\n",
    "os.makedirs('tmp/pet', exist_ok=True)\n",
    "os.makedirs('ws_mask/amazonas', exist_ok=True)\n",
    "\n",
    "d0, d1 = '2000-03-01 12:00:00', '2018-12-31'\n",
    "x_range = ((0.1, 1e4), (-1, 1), (0.1, 1e3), (0.1, 1e2))\n",
    "draws = 100\n",
    "warmup = 12 * 30 * 24 * 2 # one year in 30min steps\n",
    "n_pdf = 10\n",
    "\n",
    "x_pdf = {}\n",
    "for down_label in labels_with_vs_tree:\n",
    "    vs = labels_with_vs[down_label]\n",
    "    for s in vs:\n",
    "        print(df_vs.query(f\"station == '{s}'\").iloc[0])\n",
    "    # get whole basin's labels\n",
    "    whole_labels = startswith_label(down_label, all_labels)\n",
    "    # copy basin's masks locally\n",
    "    for label in whole_labels:\n",
    "        if not os.path.exists(f'ws_mask/amazonas/{label}'):\n",
    "            fs = gcsfs.GCSFileSystem(project='pangeo-data')\n",
    "            gcs_get_dir(f'pangeo-data/gross/ws_mask/amazonas/{label}', f'ws_mask/amazonas/{label}', fs)\n",
    "    # get upstream basin's labels and compute its area\n",
    "    # also compute upstream basins' areas\n",
    "    up_labels, areas_up = {}, {}\n",
    "    for label in labels_with_vs_tree[down_label]['up']:\n",
    "        areas_up[label] = 0\n",
    "        up_labels[label] = startswith_label(label, all_labels)\n",
    "        for label2 in up_labels[label]:\n",
    "            areas_up[label] += xr.open_zarr(f'ws_mask/amazonas/{label2}', auto_chunk=False)['mask'].attrs['area']\n",
    "    # get downstream bassin's labels and compute its area\n",
    "    down_labels = whole_labels\n",
    "    for labels in up_labels.values():\n",
    "        down_labels = subtract_label(labels, down_labels)\n",
    "    area_down = 0\n",
    "    for label in down_labels:\n",
    "        area_down += xr.open_zarr(f'ws_mask/amazonas/{label}', auto_chunk=False)['mask'].attrs['area']\n",
    "    areas = list(areas_up.values()) + [area_down]\n",
    "    area_whole = sum(areas)\n",
    "\n",
    "    if is_pangeo_data:\n",
    "        trmm_mask_path = 'gs://pangeo-data/gross/ws_mask/amazonas/trmm_mask'\n",
    "        gpm_mask_path = 'gs://pangeo-data/gross/ws_mask/amazonas/gpm_mask'\n",
    "    else:\n",
    "        trmm_mask_path = 'ws_mask/amazonas/trmm_mask'\n",
    "        gpm_mask_path = 'ws_mask/amazonas/gpm_mask'\n",
    "    da_trmm_mask = xr.open_zarr(get_path(trmm_mask_path))['mask']\n",
    "    da_gpm_mask = xr.open_zarr(get_path(gpm_mask_path))['mask']\n",
    "    p = get_precipitation(d0, d1, all_labels, da_trmm_mask, da_gpm_mask, chunk_time=True, zarr_path='ws_precipitation')\n",
    "    sys.exit()\n",
    "    \n",
    "    # get whole basin's mask\n",
    "    #da = get_mask('ws_mask/amazonas', whole_labels)\n",
    "    #subprocess.check_call('rm -rf tmp/mask'.split())\n",
    "    #da.to_dataset(name='mask').to_zarr('tmp/mask')\n",
    "    # get basin's water level time series at virtual station (basin's outlet)\n",
    "    he = get_waterlevel(d0, d1, labels_with_vs[down_label][0]) # there might be several stations\n",
    "    dh0 = he.dropna().index[0] # first date of observation\n",
    "    dh1 = he.dropna().index[-1] # last date of observation\n",
    "    # start date which allows to warmup the model, but not more (warmup is in 30min)\n",
    "    dh0 = max(str2datetime(d0), dh0 - timedelta(minutes=warmup*30))\n",
    "    # get whole basin's precipitation and PET time series\n",
    "    if False:#os.path.exists(f'tmp/precipitation/{down_label}.pkl'):\n",
    "        p_whole = pd.read_pickle(f'tmp/precipitation/{down_label}.pkl')\n",
    "        e_whole = pd.read_pickle(f'tmp/pet/{down_label}.pkl')\n",
    "    else:\n",
    "        print(f'Getting precipitation and PET for {down_label}')\n",
    "        p_whole = get_precipitation(d0, d1, mask_path)\n",
    "        p_whole.to_pickle(f'tmp/precipitation/{down_label}.pkl')\n",
    "        e_whole = get_pet(d0, d1, 'tmp/mask')\n",
    "        e_whole.to_pickle(f'tmp/pet/{down_label}.pkl')\n",
    "    p_up, e_up = {}, {}\n",
    "    # precipitation and PET of upstream bassins already exist from previous iteration\n",
    "    for label in labels_with_vs_tree[down_label]['up']:\n",
    "        p_up[label] = pd.read_pickle(f'tmp/precipitation/{label}.pkl')\n",
    "        e_up[label] = pd.read_pickle(f'tmp/pet/{label}.pkl')\n",
    "    # compute precipitation and PET of downstream bassin\n",
    "    p_down = p_whole * area_whole\n",
    "    e_down = e_whole * area_whole\n",
    "    for label in labels_with_vs_tree[down_label]['up']:\n",
    "        p_down -= p_up[label] * areas_up[label]\n",
    "        e_down -= e_up[label] * areas_up[label]\n",
    "    p_down /= area_whole\n",
    "    e_down /= area_whole\n",
    "\n",
    "    pe = []\n",
    "    # upstream basins' precipitation and PET\n",
    "    for label in labels_with_vs_tree[down_label]['up']:\n",
    "        df = DataFrame()\n",
    "        df['p'] = p_up[label].loc[dh0:dh1]\n",
    "        df['e'] = e_up[label].loc[dh0:dh1]\n",
    "        pe.append(df)\n",
    "    # downstream basin's precipitation and PET\n",
    "    df = DataFrame()\n",
    "    df['p'] = p_down.loc[dh0:dh1]\n",
    "    df['e'] = e_down.loc[dh0:dh1]\n",
    "    pe.append(df)\n",
    "    # basin's water level\n",
    "    he = he.reindex(df.index)\n",
    "\n",
    "    if not up_labels:\n",
    "        # this is a source basin (no basin flowing into it)\n",
    "        x = [dist.uniform_pdf(*r) for r in x_range]\n",
    "        prior_logp = get_prior_logp(x)\n",
    "        model_logp = get_likelihood_logp(gr4hh, warmup, pe, areas, he=he)\n",
    "    else:\n",
    "        # there are basins flowing into this basin\n",
    "        x = [xp + [dist.uniform_pdf(*d_range)] for xp in x_pdf[label] for label in labels_with_vs_tree[down_label]['up']] + [dist.uniform_pdf(*r) for r in x_range]\n",
    "        prior_logp = get_prior_logp(x)\n",
    "        model_logp = get_likelihood_logp(gr4hh, warmup, pe, areas, he=he)\n",
    "    # run SMC\n",
    "    posterior, q_sims = smc.smc(x, model_logp, prior_logp, draws=draws, dask_client=client)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    for q_sim in q_sims:\n",
    "        plt.plot(q_sim.index[warmup:], dist_map(q_sim.values[warmup:], he.h.values[warmup:]), alpha=0.005, color='blue')\n",
    "    plt.scatter(he.index[warmup:], he.h.values[warmup:])\n",
    "    plt.show()\n",
    "    # get simulated streamflow's PDF\n",
    "    if up_labels:\n",
    "        # reduce model\n",
    "        q_pdf = np.empty((2, n_pdf, q_sims.shape[1]))\n",
    "        for i in range(q_pdf.shape[2]):\n",
    "            q_pdf[:, :, i] = dist.pdf_from_samples(q_sims[:, i], nb=n_pdf, kde=True)\n",
    "        pe = DataFrame()\n",
    "        pe['p'] = p_whole.loc[dh0:dh1]\n",
    "        pe['e'] = e_whole.loc[dh0:dh1]\n",
    "        x = [dist.uniform_pdf(*r) for r in x_range]\n",
    "        prior_logp = get_prior_logp(x)\n",
    "        model_logp = get_likelihood_logp(gr4hh, warmup, pe, [1], q_pdf=q_pdf)\n",
    "        posterior, _ = smc.smc(x, model_logp, prior_logp, draws=draws, dask_client=client)\n",
    "    x_pdf[down_label] = [dist.pdf_from_samples(posterior[:, i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sel(label='0').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "ddf = dd.read_csv('/home/david/Downloads/*.csv').set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf[ddf.columns[:10]].mean(axis=1).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf[ddf.columns[:10]].compute()\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(legend=False, alpha=0.1, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_pangeo_data:\n",
    "    mask_path = 'gs://pangeo-data/gross/ws_mask/amazonas'\n",
    "else:\n",
    "    mask_path = 'ws_mask/amazonas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_trmm_mask = get_trmm_masks(mask_path, all_labels, None).astype('float32').chunk({'label': 10})\n",
    "da_trmm_mask.to_dataset(name='mask').to_zarr('trmm_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_gpm_mask = get_gpm_masks(mask_path, all_labels, None).astype('float32').chunk({'label': 10})\n",
    "da_gpm_mask.to_dataset(name='mask').to_zarr('gpm_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult(x):\n",
    "    return x * np.random.rand()\n",
    "da_trmm_mask.groupby('label').apply(mult).sum('label').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_gpm_mask.groupby('label').apply(mult).sum('label').plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
